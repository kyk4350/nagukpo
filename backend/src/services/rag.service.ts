/**
 * RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
 * Pinecone ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì œë¥¼ ê²€ìƒ‰í•˜ì—¬ GPT-4ì—ê²Œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
 */

import { Pinecone } from '@pinecone-database/pinecone'
import { OpenAIEmbeddings, ChatOpenAI } from '@langchain/openai'
import { PineconeStore } from '@langchain/pinecone'
import { RetrievalQAChain } from 'langchain/chains'
import { PromptTemplate } from '@langchain/core/prompts'
import logger from '../utils/logger'

/**
 * Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì‹±ê¸€í†¤)
 */
let pineconeClient: Pinecone | null = null
let vectorStore: PineconeStore | null = null

async function getPineconeClient(): Promise<Pinecone> {
  if (pineconeClient) {
    return pineconeClient
  }

  const apiKey = process.env.PINECONE_API_KEY

  if (!apiKey) {
    throw new Error('PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
  }

  pineconeClient = new Pinecone({
    apiKey,
  })

  return pineconeClient
}

/**
 * Pinecone Vector Store ì´ˆê¸°í™”
 */
async function getVectorStore(): Promise<PineconeStore> {
  if (vectorStore) {
    return vectorStore
  }

  const pinecone = await getPineconeClient()
  const indexName = process.env.PINECONE_INDEX_NAME || 'nagukpo-embeddings'
  const index = pinecone.Index(indexName)

  const embeddings = new OpenAIEmbeddings({
    openAIApiKey: process.env.OPENAI_API_KEY,
    modelName: 'text-embedding-3-small',
  })

  vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
    pineconeIndex: index,
    namespace: 'problems',
  })

  return vectorStore
}

/**
 * ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì œ ê²€ìƒ‰
 */
export async function searchRelevantProblems(query: string, topK: number = 3) {
  try {
    const store = await getVectorStore()

    // ìœ ì‚¬ë„ ê²€ìƒ‰
    const results = await store.similaritySearchWithScore(query, topK)

    logger.debug(`RAG ê²€ìƒ‰ ê²°ê³¼: ${results.length}ê°œ ë¬¸ì„œ ë°œê²¬`)

    return results.map(([doc, score]) => ({
      content: doc.pageContent,
      metadata: doc.metadata,
      similarity: score,
    }))
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    const errorStack = error instanceof Error ? error.stack : ''
    logger.error(`RAG ê²€ìƒ‰ ì‹¤íŒ¨: ${errorMessage}`, errorStack)
    throw error
  }
}

/**
 * RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
 */
export async function generateAnswerWithRAG(
  question: string,
  conversationHistory?: { role: string; content: string }[]
): Promise<string> {
  try {
    // 1. ê´€ë ¨ ë¬¸ì œ ê²€ìƒ‰
    const relevantDocs = await searchRelevantProblems(question, 3)

    logger.info(`RAG: ${relevantDocs.length}ê°œì˜ ê´€ë ¨ ë¬¸ì œ ê²€ìƒ‰ë¨`)

    // 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const context = relevantDocs
      .map((doc, idx) => {
        const meta = doc.metadata as any
        return `
[ì°¸ê³  ë¬¸ì œ ${idx + 1}] (ìœ ì‚¬ë„: ${(doc.similarity * 100).toFixed(1)}%)
- ë ˆë²¨: ${meta.level}
- ìœ í˜•: ${meta.type}
- ì§ˆë¬¸: ${meta.question}
- ì •ë‹µ: ${meta.answer}

${doc.content}
        `.trim()
      })
      .join('\n\n---\n\n')

    // 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    const systemPrompt = `ë‹¹ì‹ ì€ í•œêµ­ì–´ êµ­ì–´ í•™ìŠµì„ ë•ëŠ” ì¹œì ˆí•œ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.

**ì—­í• :**
- í•™ìƒì´ êµ­ì–´ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  í’€ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”
- í‹€ë¦° ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì™œ í‹€ë ¸ëŠ”ì§€ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ê´€ë ¨ ê°œë…ì„ ì‰¬ìš´ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ê²©ë ¤ì™€ ì¹­ì°¬ì„ ì•„ë¼ì§€ ë§ˆì„¸ìš”

**ë§íˆ¬:**
- ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë˜, ì¹œê·¼í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ì„œ ì¬ë¯¸ìˆê²Œ í•´ì£¼ì„¸ìš”
- ì§§ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”

**ì œì•½ì‚¬í•­:**
- êµ­ì–´ í•™ìŠµê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” "êµ­ì–´ ê³µë¶€ì— ê´€í•´ ë¬¼ì–´ë´ì£¼ì„¸ìš” ğŸ˜Š"ë¼ê³  ë‹µí•˜ì„¸ìš”
- ë‹µë³€ì€ ìµœëŒ€ 500ìë¡œ ì œí•œí•´ì£¼ì„¸ìš”

**ì°¸ê³  ë¬¸ì œ ë°ì´í„°:**
ë‹¤ìŒì€ í•™ìƒì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì…ë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš° ì°¸ê³ í•˜ì„¸ìš”.

${context}

ìœ„ ì°¸ê³  ë¬¸ì œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. í•˜ì§€ë§Œ ì°¸ê³  ë¬¸ì œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , í•™ìƒì˜ ì§ˆë¬¸ì— ë§ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`

    // 4. GPT-4ë¡œ ë‹µë³€ ìƒì„±
    const model = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'gpt-4',
      temperature: 0.7,
      maxTokens: 1500,
    })

    const messages = [
      { role: 'system', content: systemPrompt },
      ...(conversationHistory || []).slice(-6), // ìµœê·¼ 3í„´ë§Œ í¬í•¨
      { role: 'user', content: question },
    ]

    const response = await model.invoke(
      messages.map((msg) => ({
        role: msg.role as 'system' | 'user' | 'assistant',
        content: msg.content,
      }))
    )

    const answer = response.content.toString()

    logger.info('RAG ë‹µë³€ ìƒì„± ì™„ë£Œ')

    return answer
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    const errorStack = error instanceof Error ? error.stack : ''
    logger.error(`RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: ${errorMessage}`, errorStack)
    throw error
  }
}

/**
 * íŠ¹ì • ë¬¸ì œì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìƒì„±
 */
export async function explainProblem(problemId: string, userAnswer?: string): Promise<string> {
  try {
    // 1. í•´ë‹¹ ë¬¸ì œì™€ ìœ ì‚¬í•œ ë¬¸ì œë“¤ ê²€ìƒ‰
    const searchQuery = `ë¬¸ì œ ID: ${problemId}`
    const relevantDocs = await searchRelevantProblems(searchQuery, 5)

    // 2. í•´ë‹¹ ë¬¸ì œ ì°¾ê¸°
    const targetProblem = relevantDocs.find(
      (doc) => (doc.metadata as any).problemId === problemId
    )

    if (!targetProblem) {
      throw new Error('ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    }

    const meta = targetProblem.metadata as any

    // 3. ì„¤ëª… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    const prompt = `ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•´ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

**ë¬¸ì œ ì •ë³´:**
- ë ˆë²¨: ${meta.level}
- ìœ í˜•: ${meta.type}
- ì§ˆë¬¸: ${meta.question}
- ì •ë‹µ: ${meta.answer}
${userAnswer ? `- í•™ìƒ ë‹µë³€: ${userAnswer}` : ''}

${targetProblem.content}

**ì„¤ëª… ìš”ì²­:**
${
  userAnswer && userAnswer !== meta.answer
    ? `í•™ìƒì´ "${userAnswer}"ì„(ë¥¼) ì„ íƒí–ˆëŠ”ë° í‹€ë ¸ìŠµë‹ˆë‹¤. ì™œ í‹€ë ¸ëŠ”ì§€, ê·¸ë¦¬ê³  ì™œ "${meta.answer}"ì´(ê°€) ì •ë‹µì¸ì§€ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`
    : `ì´ ë¬¸ì œì˜ ì •ë‹µì¸ "${meta.answer}"ì— ëŒ€í•´ ì™œ ê·¸ê²ƒì´ ì •ë‹µì¸ì§€ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`
}

ë‹¤ë¥¸ ì„ íƒì§€ë“¤ì€ ì™œ ì˜¤ë‹µì¸ì§€ë„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ê´€ë ¨ëœ êµ­ì–´ ê°œë…ì´ë‚˜ íŒì´ ìˆë‹¤ë©´ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.`

    const model = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'gpt-4',
      temperature: 0.7,
      maxTokens: 2000,
    })

    const response = await model.invoke([
      {
        role: 'system',
        content: 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ êµ­ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. í•™ìƒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.',
      },
      { role: 'user', content: prompt },
    ])

    return response.content.toString()
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error)
    const errorStack = error instanceof Error ? error.stack : ''
    logger.error(`ë¬¸ì œ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: ${errorMessage}`, errorStack)
    throw error
  }
}
